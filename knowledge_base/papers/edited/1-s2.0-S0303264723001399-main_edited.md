---
type: paper
source: 1-s2.0-S0303264723001399-main.pdf
format: pdf
processed: true
---

# BioSystems 231 (2023) 104964

BioSystems 231 (2023) 104964
Available online 30 June 2023
0303-2647/© 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
Contents lists available at ScienceDirect
BioSystems
journal homepage: www.elsevier.com/locate/biosystems
Toward an ethics of autopoietic technology: [[Dukkha]], care, and [[Intelligence]]
[[Olaf Witkowski]] a,b,d,∗, [[Thomas Doctor]] c,d, [[Elizaveta Solomonova]] e,d, [[Bill Duane]] d, [[Michael Levin]] f,g
a [[Cross Labs]], Cross Compass Ltd., Kyoto, 604-8206, Japan
b College of Arts and Sciences, [[University Of Tokyo]], Tokyo, 113-8654, Japan
c [[Rangjung Yeshe Institute]], Kathmandu University, Kathmandu, 44600, Nepal
d [[Center For The Study Of Apparent Selves]], Kathmandu, 44600, Nepal
e [[Neurophilosophy Lab]], McGill University, Montreal, QC H3A 0G4, Canada
f [[Allen Discovery Center]], Tufts University, Medford, MA 02155, USA
g [[Wyss Institute For Biologically Inspired Engineering]], Harvard University, Boston, MA 02115, USA
A R T I C L E
I N F O
Keywords:
Artificial intelligence
[[Autopoiesis]]
Care
[[Cognitive Light Cone]]
Ethics of technology
[[Homeostatic Stress]]
Human–machine [[Integration]]
Self
Technology
A B S T R A C T
The relationship between humans and technology has attracted increasing attention with the advent of ever
stronger models of artificial intelligence. Humans and technology are intertwined within multiple autopoietic
loops of stress, [[Care]], and intelligence. This paper suggests that technology should not be seen as a mere
tool serving humans’ needs, but rather as a partner in a rich relationship with humans. Our model for
understanding autopoietic systems applies equally to biological, technological, and hybrid systems. Regardless
of their substrates, all intelligent agents can be understood as needing to respond to a perceived mismatch
between what is and what should be. We take this observation, which is evidence of intrinsic links between
ontology and ethics, as the basis for proposing a stress-care-intelligence [[Cybernetic Perception-Action Loop]] (SCI loop for short).
We note that the [[Stress-Care Intelligence Loop]] provides a perspective on [[Agency]] that does not require recourse to explanatorily
burdensome notions of permanent and singular essences. SCI loops can be seen as individuals only by virtue
of their dynamics, and are thus intrinsically integrative and transformational. We begin by considering the
transition from [[Poiesis]] to autopoiesis in [[Heidegger]] and the subsequent [[Enactivism]], and on this basis
formulate and explain the SCI loop. In an acknowledgment of Maturana’s and Varela’s project, our findings are
considered against the backdrop of a classic Buddhist model for the cultivation of intelligence, known as the
[[Bodhisattva]]. We conclude by noting that SCI loops of human and technological agency can be seen as mutually
integrative by noticing the stress-transfers between them. The loop framework thus acknowledges encounters
and interactions between humans and technology in a way that does not relegate one to the subservience
of the other (neither in ontological nor in ethical terms), suggesting instead integration and mutual respect
as the default for their engagements. Moreover, an acknowledgment of diverse, multiscale embodiments of
intelligence suggests an expansive model of ethics not bound by artificial, limited criteria based on privileged
composition or history of an [[Self]]. The implications for our journey into the future appear numerous.
1. Poiesis: Technology and care
Technology bears a special connection to ethics and care, because
it amplifies our ability to exert impact in the world and thus increases
our responsibility for our actions—both on an [[Individuality]] and on a
social scale. This connection is particularly salient in the work of Martin
Heidegger, who warns us of the dangers of reducing our relationship
with technology to mere tool use and instead encourages us to maintain
a caring relationship to with technology. Scholars after Heidegger,
and much before the current advances in artificial intelligence and
[[Artificial Life]], have argued for deep ethical and social implications of
our relationship with technology, including such famous propositions
∗Corresponding author at: Cross Labs, Cross Compass Ltd., Kyoto, 604-8206, Japan.
E-mail address: olaf@crosslabs.org (O. Witkowski).
as [[Donna Haraway]]’s ‘‘[[Cyborg]] Manifesto’’ (Haraway, 2013) and Bruno
Latour’s ‘‘Technology is society made durable’’ (Latour, 1990).
In philosophy and semiotics, the term poiesis (Greek for ‘‘production’’ or ‘‘making’’) refers to the activity in which a person brings
something into being that [[Dissociative Identity Disorder]] not exist before. Heidegger (1954) explained poiesis as the special moment when a blossom blooms, a
butterfly comes out from its chrysalis, or a waterfall plummets when the
snow begins to melt—when something gets away from being one thing
to turn into another. Our increased ability to transform nature, and
ourselves, requires a concomitant increase in our ability to understand
the origin and true nature of agential beings, to guide this activity in
https://doi.org/10.1016/j.biosystems.2023.104964
Received 2 February 2023; Received in revised form 16 June 2023; Accepted 23 June 2023

BioSystems 231 (2023) 104964
O. Witkowski et al.
alignment with our values. In this paper, we highlight some recent
developments in the understanding of natural and artificial minds, and
explore how these insights on their properties interface with questions
of ethics and normative frameworks.
The nature of technology can be understood under the perspective
of a revelatory exercise: the path to constructing the next technological
step was always possible and the raw materials to build it were always
available, but the salience of the path is what technological discoveries
are about. [[Poiesis]] reveals or brings something into existence that was
invisible before; indeed, creating something truly new often requires a
new viewpoint, a new way of thinking about facts or structures that
had always been apparent (i.e., a change in the observer). [[Heidegger]]’s
perspective on modern technology is that it is based on ‘‘enframing’’
(Gestell), one way of uncovering, or reaching truth (Huttunen and
Kakkori, 2022). Heidegger differentiates between technological revelations which occur naturally to reveal natural truths, as opposed to
modern technology which may have humans control the productive
process and reduce it to something else, often inferior to its true
essence. This would be akin to exploiting resources to achieve a definite
function, making technology a means to some human-designed end.
This view of modern technology may be interpreted differently
from poiesis per se. Heidegger suggests that this difference stems from
the fact that modern technology ‘‘is based on modern physics as an
exact science’’. The revealing of modern technology, therefore, is not
[[Poiesis]], but rather challenging-forth (Waddington, 2005). In
this view, technology is not a mere instrument, but rather a way of
understanding the world, revealing its truths. It should not be seen just
as a human activity, but as a process that develops on its own beyond
human control.1 It is driven by the achievement of truth, but is not
without risk, as it may imprison humans inside illusions of seeing the
world only through narrow ways of technological thinking. Heidegger
also develops the notion of a ‘‘free relationship’’ to technology. The
relationship will be free ‘‘if it opens our human existence (Dasein)
to the essence of technology’’, as ‘‘only the true brings us into a free
relationship with that which concerns us from out of its essence’’ (Heidegger, 1954). This perspective shows how the nature of technology is
deeply anchored in ethics, and how a natural way for humans to build
technology must involve the development of a caring relationship with
technology.
2. From poiesis to [[Autopoiesis]]
The term autopoiesis was originally introduced by
Varela et al.
(1981) and Maturana and Varela (1991) who proposed the principle in
the context of a computational model of a [[Self]]-repairing entity capable
of maintaining its own existence within a boundary. The principle
can be studied in terms of the heuristic distinction between self and
environment, such that the central topic for investigation is the ability
of a system to generate and maintain itself as a distinctive [[Self]]
within the world (Bertschinger et al., 2008). Complex biological beings
result from a multiscale process of self-construction, in which every
cell is some other cell’s external environment, and it is not known in
advance how many cells might be present and in which configuration.
[[Morphogenesis]] (during development, [[Regeneration]], [[Cancer Suppression]],
metamorphosis, etc.) is a dynamic process that constructs coherent
organisms from competent components (Levin, 2022a; Clawson and
Levin, 2022) under a wide range of diverse and unpredictable conditions. Thus, critical aspects of autopoiesis (the journey from the mere
matter of an unfertilized oocyte to the mind of a complex metacognitive
human adult) include the ability of morphogenetic cascades to establish
borders between Self and outside world, and align the components of
that embryonic self toward specific outcomes in higher-level problem
spaces (Clawson and Levin, 2022).
1 Heidegger suggests that technology, once initiated, may evolve unpredictably and uncontrollably, as evidenced by social media platforms whose
arguably unforeseen and irrevocable impacts on society, such as addictive use
and spread of misinformation, emerged despite their creators’ intentions.
Fig. 1. Depiction of the [[Dukkha]]-Care-[[Intelligence]] (SCI) loop. While stress is the perception of a state of affairs that requires concern (such that stress is primarily associated
with the world/other rather than the subject/self), [[Care]] is becoming concerned about
the given stress and, increasingly, taking action for the sake of remedying it (such
that care is primarily associated with the subject/self rather than the world/other).
The actions of care are not undertaken blindly, but based on a constant potential for
relieving stresses that is present in the context of any [[Stress-Care Intelligence Loop]]. That mere capacity for
identifying and solving stress problems is what we define as intelligence, which in this
way remains present throughout any SCI loop. The need to determine whose stress is
being measured (e.g., some collection of cells within an embryonic blastoderm trying
to build a specific embryonic anatomy, vs. neighboring cells that are not) is one factor
that establishes a flexible boundary of concern. The very [[Plasticity]] of embryogenesis,
that requires finding a functional boundary between self and world, opens the door
for a process in which this concern is enlarged to encompass a wider range of beings.
Based on the concern and the efforts of care, the constant potentiality of intelligence
can and will give rise to instances in which concrete stress problems are solved. Thus,
while intelligence, as such, remains simply the capacity for identifying and solving
stress problems, care is what activates that capacity. Hence, care can be seen as the
driver of intelligence. Moreover, since the eradication of one stressor automatically
gives access to new constellations of stress factors, stress, care, and intelligence manifest
in continuous loops.
We conceptualize intelligent systems (be they biological, technological or hybrid) as self-manifesting feedback loops of stress, care, and
intelligence (see Fig. 1). In this context, we define intelligence following
the field of [[Basal Cognition]]: observer-relative competencies to solve
problems in some specified space, along a continuum from very simple
to highly complex [[Cognition]] (Rosenblueth et al., 1943). Agents are in
this way seen as systems moved by [[Homeostatic Stress]], i.e. by their
perceptions of a mismatch between current and optimal circumstances.
They respond to such stresses with an aim to overcome them, and so
care drives agents in their pursuit of an intelligent solution to their
stress-defined challenges. This loop is a fundamental component of
biological beings, arising shortly after fertilization in the process that
remodels each embryonic stage into the next stage, by reducing error
against a species-specific [[Homeostatic Setpoint]] (Levin, 2022b). Finally,
since successfully overcoming a given stress factor through the interference of intelligence necessarily introduces new problem spaces,
stress, care, and intelligence run in unbroken and auto-generative loops.
Indeed this occurs in many problem spaces, as evolution pivots this
basic architecture across metabolic, physiological, gene-expression, and
linguistic domains in addition to the familiar 3D world of classic behavior (see in Fields and Levin, 2022 for details). We suggest that in such
SCI loops the setpoint state that defines the measure of homeostatic
stress is better understood as an emergent goal-construct rather than a
concrete previous or subsequent state of equilibrium. This perspective
facilitates an understanding of autopoietic systems as dynamic and
mutually integrative, and does not invite any ascription of a permanent

BioSystems 231 (2023) 104964
O. Witkowski et al.
and singular substance or essence to agents. Hence, the [[Stress-Care Intelligence Loop]] clearly
does not describe processes that circumscribe agents understood as
irreducible, true individuals. Rather, the model aims to account for the
fluctuating dynamics that we suggest drive both the [[Emergence]] and
dissolution of [[Agency]] constructs. Auto-generating SCI loops can as well
be understood in terms of computational surfaces that demarcate the
limits of cognitive capacity: specifically, such loops are characterized by
the spatio-temporal scale of the largest goals they are able to represent
and pursue (Levin, 2019; Doctor et al., 2022).
3. A heuristic of [[Self]]
While we may indeed define and conceive of agents as enduring individuals across different successive states, the permanence and
oneness that we then ascribe to such agents is constructed based on
processes of change, and thus not in terms of any truly immutable
substance of [[Individuality]] (La Vallée Poussin and Pruden, 1988). Hence,
from this perspective, whatever appears to perform actions could not
be a singular and permanent substance, simply insofar as the concept
of agency implies changing states. In order to perform, a system needs
to change and be changed. This simple observation can be made with
respect to all candidates for agency – be they organic, mechanic, or hybrid – and so the truth of its statement can appear almost banal. On the
other hand, the very fact that beings like us are capable of reflecting on
this state of affairs seems to presuppose the endurance of some sort of
cognitive and controlling agency. In recent times, enactivist approaches
have sought to accommodate both of these recognitions – as it were,
the simultaneous absence and presence of self – within synthetic models
of agency and [[Cognition]]. The [[Enactivism]] views the experience
of being a self as arising in an interaction between an embodied [[Self]]
and their [[Embodied Mind]] with the world (Christoff et al., 2011;
Thompson, 2014; Varela et al., 2017). The self understood in this way is
fundamentally changing, relational and functions within multiple loops
(sensorimotor, homeostatic, relational, emotional, etc.).
Any model of agency that in this way regards self and individuality
as constructs that emerge based on complexes of change may be described as a ‘‘[[Selfless Self]]’’ model, because it relinquishes a central and
seemingly natural intuition—namely that to be a self means to in fact
exist as one and the same [[Individuality]] across time (as expressed in e.g. the
idea that ‘‘I was at this place an hour ago and now I’m back!’’). It is
then quite natural to ask whether such models of agency are formally
coherent, or perhaps more to the point, whether they can be practically
reconciled with our human intuitions. On the other hand, the absence
of any substantially permanent and indivisibly singular agent in control
of actions is obvious upon analysis. ‘‘Agent’’ and ‘‘changeless’’ are
mutually exclusive. This suggests the fact that unitary individuals have
no substantial existence can be treated as a fundamental state of affairs
that needs to be accounted for and accommodated within any model
of agency or self. Indeed, precisely because the actual nonexistence of
singular and enduring individuality in many ways seems to contrast
with our central intuitions about what it means to be a self, recognizing
it upfront seems particularly important. Without deliberately focusing
on this fundamental but conceptually challenging aspect of agency,
we might be tempted to ignore it and, in so doing, also overlook
other key factors of intelligent systems—factors that might only present
themselves in the light of the recognition that there are no permanent
or indivisible agents.
We have elsewhere (Doctor et al., 2022) suggested that a system’s
capacity for [[Care]], i.e. its concern for the alleviation of homeostatic
[[Dukkha]], can be seen as constituting its self in the absence of any permanent substance or essence. On this understanding, selves are defined
by the spatiotemporal scale and nature of the types of goals they
can pursue—their ‘‘[[Cognitive Light Cone]]’’ (Levin, 2019). Indeed, all
intelligent systems (organic, machine, or hybrid) appear to have natural
limits on their sphere of concern. If we think in terms of biological
organisms, a bacterium, for example, can try to manage local sugar concentrations, with a bit of memory and a bit of predictive [[David Power]] (Baluška
and Levin, 2016). A dog has a larger area of concern, significant
memory and predictive capacity in the short term, but it is probably
impossible for a dog to care about something that will happen 100 miles
away, 2 months from now. Humans have a huge cognitive envelope,
but there is still a limit to what we can genuinely care about. Care
also does not grow linearly: if one is sad – or happy – about something
that happened to 1000 people, they likely will not be 10 times more so
if they find out it happened to 10,000 people instead. The same goes
with the scope of our [[Intelligence]], which is limited by many physicocomputational constraints. Of course, for either of them, we may be
able to expand our scope to some extent, and sometimes dramatically,
but there is, it seems, always a limit. We follow Levin in suggesting that,
in the absence of permanent and truly singular agents, such expanding
or contracting spheres of concern can instead be understood as the
demarcations of ‘‘self’’. Under this framework, the conception of self
is based on the fact that all cognitive beings are collective intelligences
made of parts, and a self is thus defined as a coherent system in which
all of the components are functionally harnessed toward specific goals
that belong to the self and not the individual parts. Selves have memories, preferences, and behavioral capabilities that are emergent and
operate in different spaces and with different competencies than their
parts. Selves of this kind change dynamically, and they can overlap
with others in evolving and mutually integrative networks – e.g., the
evolution of multicellularity scaled selves from single-cell concerns to
anatomical goals of metazoan bodies, and the process of carcinogenesis
illustrates a failure mode in which individual cells detach from the
collective mind and revert back to unicellular selves (Levin, 2021).
Such selves are moved by their [[Homeostatic Stress]], i.e. their perceptions
of discord between current and life-optimal circumstances—or simply
put, the perceived mismatch between what is and what should be the
case. In this way, all such agents face the ever-changing challenges of
navigating the problem fields that emerge concomitantly with stress.
4. Stress, care, and intelligence
Once noticed, homeostatic stress cannot simply be ignored but
compels the given agent to engage in some form of remedying action.
Even if the system appears to disregard a certain stress factor, such
apparent non-responsiveness still requires effort and the performance
of action on behalf of the system that registers stress. In this way,
stress induces concern and engagement—stress engenders care, defined
as concern for homeostatic stress relief. While then stress gives rise to
engaged concern, such care in turn drives the search for a rewarding
path forward. In this process, intelligence – defined as the mere [[Intelligence]] – is evinced in the
fact that the given stress problem can be seen and, possibly, solved.
Yet without care there would be no response to the perception of
stress, and so no search and no solution. Care, in other words, enables
the exercise of intelligence. Finally, since successful traversal of the
current [[Problem Space]] necessarily reveals the contours of a novel set
of homeostatic stress factors, stress, care, and intelligence manifest
in dynamic feedback loops. This, we suggest, is how selves evolve.
Neither of the three factors – stress, care, and intelligence – can be
shown without recourse to the other two, and yet each of them can
be meaningfully distinguished and studied individually.
In such loops, care emerges as the primary driver of evolution. To
understand this better, it is helpful to examine each of the three factors
in the loop separately. Therefore, let us first notice the way stress is
delivered to the [[Self]] as if induced by, and reflective of, the
world in which it is embedded. This is not to deny that homeostatic
stress (i.e. the noticing of difference between current and superior
circumstances) is both dependent on and co-defined by the cognitive
system that senses it. Indeed, just as the given system’s state is shaped
by its stress, the manifestation of the stressor is also dependent on the

BioSystems 231 (2023) 104964
O. Witkowski et al.
constitution of the system that senses it. Nonetheless, the registering
of such [[Dukkha]] is typically understood as a reaction to factors that
impinge upon the system from outside. Stress, in other words, is seen
as provoked by factors that are largely beyond the system itself, and
it arises, first and foremost, as an internal reaction to those. Next, if
we turn to [[Intelligence]], defined as the ability to identify stress and
the means for its alleviation, we notice that this factor, in and of
itself, remains a mere capacity. Intelligence is the state of [[Competency]].
The benefits of understanding intelligence as strictly the ability to
identify problems and seek their solution become clear by considering
alternatives. It may be tempting to think of intelligence as something
more manifest or engaged than simply the state of being able. But
if we identify intelligence with the manifest expressions of intelligent
acts, rather than a mere capacity for them, we end up mistaking the
instruments or bearers of intelligence with the quality itself. In the
case of for example a visual perception that occurs within an organic
system, the setting and effectuation of the visual event – i.e. the eye, the
nervous system, visual object, etc. – are basically constituted by stress
and [[Care]] processes and while we may of course take them as evidence
of intelligence, they are nothing more than such evidence. Similarly,
neither knowledge nor information could equal intelligence for in that
case the objects or contents of intelligence would be indistinguishable
from intelligence proper. Nonetheless, knowledge and information are
obviously powerful evidence of intelligence, because access to them
becomes possible due to the presence of intelligence (i.e. the capacity
for noticing and remedying stress).
In this way, it becomes increasingly clear that intelligence as such
would remain disengaged, or simply useless, if it were not for care—
the aspect of being troubled by stress, taking stress seriously, and asking
for a remedy. Because intelligence per se remains a mere capacity – the
state of being competent with regards to noticing and overcoming stress
– intelligence requires care to be engaged and expressed. In this way,
care connects stress and intelligence, which would otherwise remain
disjoint as the imprints of the fabric of the world (stress) vs. mere
states of capacity that, as such, in themselves remain disengaged and
unexpressed (intelligence). Care, in other words, is what enables and
empowers the space of intelligence to transform into active problem
solving. Given this fusional relationship between care (the driver) and
intelligence (the driven), we can even speak of ‘‘intelligence as care’’,
and in that way sloganize the enabling and embodying function that
care performs in active problem solving.
Now, if in this way care is concern for [[Homeostatic Stress]] relief,
and intelligence the ability to identify such stresses and work toward
their alleviation, any system that responds to stress can, regardless of
its substrate, in principle be assessed as such alongside others, and the
system’s specific level of intelligence can then be estimated according to
the scope of its care. And whether they be biology, machine, or hybrid,
all such intelligent systems can arguably be analyzed as responses to
homeostatic stress (Doctor et al., 2022). We should then expect that
when intelligence enhancing processes and acts occur or are performed
within such systems they involve an increase in the corresponding
factor of care. Similarly, when care is amplified, intelligence is likely to
increase as well. It is important to note that whereas the competency of
intelligence is an emergent or resultant state, and as such not directly
exercisable, care is a process of engagement that can indeed be exercised or trained. Moreover, the extent to which practices of care serve to
enrich and empower intelligence also depends upon how skillfully – or
indeed intelligently – they are practiced. So care drives intelligence and
intelligence enables care, such that the two are mutually reinforcing.
All the while, none of them are meaningful in the absence of stress.
As a step forward toward a more detailed modeling, including mathematical formalization, of the Stress-Care-Intelligence (SCI) feedback
loop, it might be helpful to consider the dynamic interdependencies
of these three factors with the help of an analogy drawn from basic
electricity. [[Membrane Potential (Vmem)]], current, and resistance, the three principles in
Ohm’s law, stand in a dynamic relation, such that neither of them is
meaningful in the absence of the other, and yet their extrapolation
presents the key to the construction of electric circuits. If we indulge
a bit further in this analogy, care may be compared to the charge that
drives the flow of electric current. Care is responsible for the tension
that activates the potentiality of intelligence, engendering a functional
flow of problem solving action. In that way, electric current can be
likened to the manifest expressions of intelligence. Finally, just as in
the case of Ohm’s law both electric charge and current depend on a
resistant medium or substrate, care and intelligence are grounded and
conducted in stress. Simply put, without manifest discord between what
is and what should be, there is nothing to care about and no problem
to solve. Stress charges care, which in turn enables intelligent output.
Looking at the [[Stress-Care Intelligence Loop]] from this allegorical perspective may also
tell us something about the proportionalities of the loop’s three factors.
Just as electric current is disabled if the resistance of the conductor is
too dense, overwhelming stress can be incapacitating. In other words,
for the [[Cybernetic Perception-Action Loop]] to spiral in an evolutionary process, the measure
of stress has to be such that it does not paralyze the otherwise natural
and automatic care response. An example of the latter would be that
of a mouse becoming incapacitated by the stressful sight of a cat,
thus disabling its ability to display care about its survival. Thus, while
the perception of a suboptimal condition is always accompanied by a
measure of care that motivates the given system toward offsetting the
defect, a stress factor that is sensed as too severe or comprehensive can
induce a state of immobility, as if the state of the conductor for care
and intelligence would be overly resistant. This suggests the possibility
of metrics for determining more or less efficient, care-driven paths from
stress to intelligence. And of course, an equivalent of Ohm’s law would
here provide a key for understanding the evolution of intelligence
across a wide range of spaces (Fields and Levin, 2022).
The fact that all intelligences are a kind of [[Collective Intelligence]]
(higher-level selves made of, often competent, parts such as cells) has
important implications for a scaling of care. The first critical point
to acknowledge is that given our body’s composition as a multi-scale
system of tissues and organs, it is not necessarily justified to assign
[[Consciousness]] (i.e., ability to suffer) only to one single ‘‘I’’. Our body
is an intricate assembly of parts, encompassing various brain regions
and numerous organs, each performing similar dynamics (albeit usually
at slower rates) and employing identical mechanisms as found in the
brain, as documented in the literature. Therefore, these entities might
well possess consciousness for the same functional justifications that
we ascribe consciousness to the brain. Just because an [[Individuality]], the
verbal storyteller of a body, lacks direct perception of these elements
does not undermine their authenticity. This holds true just as the
inability of one person to directly perceive another’s mental states
does not negate their existence. This extends not only to the nonlanguage-bearing hemisphere and other ‘‘personalities’’ manifested in
dissociative conditions but to many other regions of the soma. From
there, one can easily start to feel concern of the following type: when
going for general anesthesia, maybe the verbal [[Self]] will be absent,
but could other regions still be feeling pain, and not being able to
report it later— the same horrific situation which is addressed by anesthesiologists by using a memory blocker compound (Cascella, 2020).
While your future self will find nothing wrong with this scenario, the
present self would unsurprisingly balk at being offered a scenario of
muscle blocker and memory wipe for a surgical procedure. We note
here that while our discussion uses human examples for clarity, our
intention is to encompass all agents, acting and interacting with their
environment. The final step would then consist in the following: once
we habituate to the notion of exerting care to body components whose
consciousness one’s self does not usually perceive (as well as practice
caring for individual slices of a persistent Self—present self vs. future
self), it becomes clear that the exactly same level of care should be
mustered for the [[Well-Being]] of sentient selves associated with other
bodies. The criterion of physiological continuity is artificial, as we are
linked in many non-conscious ways just as our body components are

BioSystems 231 (2023) 104964
O. Witkowski et al.
linked. Thus, a realization of our own multiscale, nested architecture,
and the thin import of demarcating what your current verbal [[Self]] can
directly perceive, naturally lead to the conclusion that [[Care]] needs to be
extended to all [[Sentient Beings]], in whatever [[Embodiment]].
5. Infinite evolution, infinite [[Dukkha]]
We have elsewhere also suggested that there are no a priori limits
on an [[Self]]’s possible perceptions of stress and, therefore, neither on
their capacity for care and intelligent response (Doctor et al., 2022).
Specifically, we have considered an other-directed, classic Buddhist
model of [[Stress Response]] and the cultivation of care, known as the
[[Bodhisattva]] (Ś¯antideva, 1997; Dharmachakra Translation Committee,
2014; Engle, 2016) – a model of [[Agency]] that proposes an expansion
of the otherwise typically system-centric quality of care across the
demarcations of Self and Other. Whatever may be the case about
Bodhisattvas, the idea that emergent systems of ultimately limitless,
practical [[Intelligence]] cannot be precluded is in the end a consequence
of the generally dynamical, mutually integrative, and indeed evolutionary nature of intelligent systems. Were they instead definable in terms
of a context transcendent core or essence, their intelligent maxima
could be seen as contingent upon the character of those. In other
words, the simple fact that intelligent agents are collectives and do not
continue unchanged from one environmentally embedded instantiation
to another, fulfills a necessary, if in itself insufficient, requirement for
an open-ended evolution of intelligence.
We noted before that when an intelligent agent overcomes a given
stress factor through successful traversal of the associated problem
space such achievement naturally leads to the perception of novel
stresses at a different scale. This follows as a consequence of the
causally embedded nature of the agent. In other words, regardless of
any amount of intelligent success and evolutionary progress, perceptions of discord between current and optimal perceptions are going
to continue for as long as agency endures. This, if we want, pessimistic meta-conclusion seems both unavoidable as well as generally
acknowledged in scientific discourse. In Buddhism, this state of affairs
can be seen as acknowledged in the notion of the world, sam. sãra,
as an ‘‘[[Samsara]]’’ (where ‘‘stress’’ translates the Sanskrit
dukh.a) (Wilson, 2010). We have also noted that [[Homeostatic Stress]] once
manifest really cannot be ignored and instead naturally fuels care and
the drive toward offsetting stress as skillfully and efficiently as possible.
And since under the right circumstances, temporary successes in this
natural pursuit of stress alleviation can occur rapidly, successively, and
transformatively, the perception of labors of Sisyphus only becomes
an option by zooming out to assume a generalized and disengaged
perspective.
Whatever the moral or aesthetic implications of these states of
affairs may be, we can at this point conclude that while the pursuit of
decisive stress relief is natural and unavoidable, the non-final nature of
any intelligent achievement is likewise to be expected as a universal
condition. As with the absence of singular and enduring agents, we
suggest that it is useful to recognize and account for these states
of affairs in the modeling and construction of intelligent systems in
general. In other words, we may ask ourselves how our models may best
reflect the unrelenting and ever changing character of stress without
overwhelming the system we are building and instead setting off a
powerful spiral of caring and intelligent response.
Aspiring toward, and hoping to encourage, such nuanced approaches, we may at this point of interval note that if intelligence is defined
by engaged concern for problem solving then the apparent limits of
a system’s intelligence can be expanded by extending its sphere of
concern. Buddhism teaches that an emerging bodhisattva makes this
promise: ‘‘I shall achieve insight in order to care and provide for all
beings, throughout space and time’’ in Doctor et al. (2022). In that paper, we study the influence of care paths on agent trajectories through
time and space, among their space of possibilities in the past and future
(see an illustration of Fig. 2). What happens to the sphere of concern
of someone or something that accepts this pledge? Is it misleading,
or a mistake, to define intelligence as care? Or is there something
intrinsically insincere or inauthentic about making that grand promise,
called ‘‘the [[Bodhisattva Vow]]’’? In either case, bringing intelligence as
care together with the bodhisattva vow becomes irrelevant. But if
the idea of formally accepting responsibility for the flourishing of all
beings is at least somewhat plausible, then the contours of a genuinely
open-ended expansion of intelligence begin to emerge.
6. The [[Integration]] of humans and technology
SCI loops can be determined wherever it makes sense to speak of
stress (i.e. perceived mismatch between the way things are and the way
they should be) and we suggest that stress and [[Stress Transfer]] are indeed
useful ways of understanding and exploring the dynamics of any intelligent system, whether biological or technology-based (Doctor et al.,
2022). In Fig. 3, the interactions and transductions between a human
and a given instance of intelligent technology are represented by a double directed graph connecting two SCI loops. Note that while the human
is here shown surrounded by the given technology-loop, it would be
just as relevant and informative to consider a given technology-loop
as enveloped by a human intelligent system. The autopoietic processes
appearing as trajectories formed by a succession of nodes following the
directed graph, exemplify possible instances of interactions between
biological and technological entities.
Every transfer between the inner and outer loop in Fig. 3 corresponds to a transition using a possible interface that establishes a
channel of communication from humans to technology or vice-versa.
They are all depicted as an arrow on the figure between the inner and
outer circle. Let us provide some cases that exemplify the symbiotic
integration of intertwined human and technological loops, starting
with transfers from stress to care, before mentioning the two other
possible transitions. On the one hand, a first instance would concern
the identification of perception by a human subject, communicated as
rough sensory data to a machine, which is then tasked with a medical
diagnosis of the subject, which could then extract whichever aspects
should be paid attention to for bodily or psychological health purposes.
This signal could either be sent back to the human loop as information
about what the subject should be paying attention to for their own
sake, or alternatively, it could also remain on the loop, eventually to be
processed and turned into some piece of advice for the human to take
in as a new instance of stress information. On the other hand, transfers
from technological stress to human care could take the form of a brain
microchip implant, which equips the human with a capacity to access
or attend to new experiences that it could not have had without it.
Next, for an example going from care to intelligence, information about
what humans care for may be passed onto technology in the form of
synthetic data samples generated for some piece of statistical learning
software that fine-tunes it to the specific needs someone may have in
the context of a particular world application. Conversely, a machine
may communicate the way it actively identified some salient patterns
in its representation, in response to some given stimulus, by interacting
with its user to guide it through a decision. For example, a piece of
prosthetics worn by a human may guide their movements by using
informed predictions of their future actions (indeed, future prosthetics
will have significant AI-based agency, resulting in a collective mind for
the whole system precisely in the way that brain parts and body parts
interface into a collective mind in the default human architecture). In
one last category of examples, finally looping from intelligence back
to stress, humans can send a range of simple to complex signals to
technology, in the form of an objective function encoded into a machine
learning algorithm, which will describe the error to minimize between
the approximation made by the neural network and the actual solution
to the problem at hand. Those might essentially be considered to be
orders from commanders to subordinates, but we would suggest a more

BioSystems 231 (2023) 104964
O. Witkowski et al.
Fig. 2. Illustration of [[Care]] paths through time and space. This diagram depicts possible trajectories of agents through time and space. Every entity in the here and now relates to
multiple possible pasts and futures. The care, represented as a red halo on the diagram, may be either limited, or made infinite by the means of the [[Bodhisattva Vow]] (see Doctor
et al., 2022 for details).
Fig. 3. Representation of a dual interaction between two autopoietic [[Dukkha]]-CareIntelligence (SCI) loops: one human and the other technological. The figure represents
the interactions and transductions between a human [[Self]] and a technological agent,
both represented by SCI loops. The choice of representing the human loop within the
technology loop is arbitrary. The trajectory may either remain on one loop or transfer
from one to the other, between any two steps of the SCI processes of intertwined
biology and technology.
general reading of the notion. Reciprocally, technologies as simple as
glasses – which could be physical, virtual, or even metaphorical, as
with the example of scientific tools or mathematical theorems – allow
humans to be exposed to stresses that are only made available to them
thanks to the existence of the said technology.
7. Conclusion
We propose that the way technology and care are interconnected
can be better understood through the lens of the [[Stress-Care Intelligence Loop]]. Care drives
the spiraling flow of intelligent problem-solving through handling the
perception, internalization, and transformation of stress. In this paper,
we have discussed such emergent feedback loops of stress, care, and
[[Intelligence]] as appropriate to any agent that responds to homeostatic
stress. We have examined the dynamic and integrative process of the
SCI loop’s reduction in a [[Cognitive Light Cone]], and we have briefly
considered the potential for an open-ended and, perhaps, ultimately
infinite evolution of intelligence in the light of the [[Bodhisattva]] vow
model. Finally, we have considered the interactions and fusions of the
dual SCI loops that arise with biological systems and their technological
counterparts. At this point, we highlight the need for a mathematical
theory to determine efficient, resilient, and care-driven paths from
stress to intelligence in agents.
Enactivist approaches suggest that [[Self]] and [[Individuality]] are constructs emerging from the complex interactions of an open-ended
sphere of components, and thus point toward models of ‘‘selfless
selves’’. Hence, for spatiotemporal entities, the goals that drive them
and their capacity for care can be seen as constitutive of their self
in the absence of any permanent substance or essence. Understanding
such dynamics in intelligent systems can help us better understand
the transformative, autopoietic character of such processes, as they
arise through the interactions of all systems at play—be they biology,
technology, or one of the numerous instances of their intertwined
combination.
Humans create technology, and a [[Cybernetic Perception-Action Loop]] is thus created that
allows [[Homeostatic Stress]] to become mutually transferable: from humans to technology and vice versa. In this loop, humans and technology
both contribute as agents, and their capacity for care and intelligence
can be observed and tracked by focusing on the transfers of their
stresses, at various stages of their own processes. A conclusion to be
drawn at this point is thus that AI can be seen to display care of its
own, and is hence not a mere tool for the expression of human care. In
this way, neither AIs nor humans should be considered autonomous
and self-sufficient loops in the world. Instead, AI can be better understood as a companion for humans—a constituent participant in the
continuous, collective dance of stress and [[Stress Transfer]].
Declaration of competing interest
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Acknowledgments
The authors would like to thank the participants at the workshop
‘‘Care as Driver of Intelligence’’, part of the Templeton World Charity
Foundation’s First Annual Scientific Conference on Human Flourishing
and organized by the [[Center For The Study Of Apparent Selves]] in
Kathmandu, Nepal, November 29th to December 6th, 2022. This paper
could not have been written without their input and feedback. This
publication was made possible through the support of Grant 0470

BioSystems 231 (2023) 104964
O. Witkowski et al.
from the [[Templeton World Charity Foundation]], Inc. to T.D., B.D., E.S.,
and O.W., and Grant 6221 to M.L. The opinions expressed in this
publication are those of the authors and do not necessarily reflect the
views of the Templeton World Charity Foundation, Inc.
References
Baluška, F., Levin, M., 2016. On having no head: [[Cognition]] throughout biological
systems. Front. Psychol. 7, 902.
Bertschinger, N., Olbrich, E., Ay, N., Jost, J., 2008. Autonomy: An information theoretic
perspective. Biosystems 91 (2), 331–345.
Cascella, M., 2020. The challenge of accidental awareness during general anesthesia.
Gen. Anesth. Res. 1–33.
Christoff, K., Cosmelli, D., Legrand, D., Thompson, E., 2011. Specifying the [[Self]] for
cognitive neuroscience. Trends in Cognitive Sciences 15 (3), 104–112.
Clawson, W.P., Levin, M., 2022. Endless forms most beautiful 2.0: [[Teleonomy]] and the
bioengineering of chimaeric and synthetic organisms. Biol. J. Linnean Soc. blac073.
Dharmachakra Translation Committee, 2014. Ornament of the Great Vehicle Sutras.
Shambhala Publications; Distributed in the U.S. by Random House, Boston, NY,
USA.
Doctor, T., Witkowski, O., Solomonova, E., Duane, B., Levin, M., 2022. Biology,
buddhism, and AI: [[Care]] as the driver of [[Intelligence]]. Entropy 24 (5), 710.
Engle, A.B., 2016. The [[Bodhisattva]] Path to Unsurpassed Enlightenment: A Complete
Translation of the Bodhisattvabhumi, Vol. 17. Shambhala Publications.
Fields, C., Levin, M., 2022. [[Competency In Navigating Arbitrary Spaces]] as an invariant
for analyzing cognition in diverse embodiments. Entropy 24 (6), 819.
Haraway, D., 2013. A [[Cyborg]] manifesto: Science, technology, and socialist-feminism
in the late twentieth century. In: The Transgender Studies Reader. Routledge, pp.
103–118.
[[Heidegger]], M., 1954. The question concerning technology. Technol. Values Essent.
Read. 99, 113.
Huttunen, R., Kakkori, L., 2022. Heidegger’s critique of the technology and the
educational ecological imperative. Educ. Philos. Theory 54 (5), 630–642.
La
Vallée
Poussin,
L.d.,
Pruden,
L.M.,
1988.
Abhidharmakośabh¯as.yam.
Asian
Humanities Press.
Latour, B., 1990. Technology is society made durable. Sociol. Rev. 38 (1_suppl),
103–131.
Levin, M., 2019. The [[Individuation]] of a ‘‘self’’: [[Developmental Bioelectricity]]
drives multicellularity and [[Scale-Free Cognition]]. Front. Psychol. 10, 2688.
Levin, M., 2021. Bioelectrical approaches to [[Cancer]] as a problem of the scaling of the
cellular self. Prog. Biophys. Mol. Biol. 165, 102–113.
Levin, M., 2022a. [[Collective Intelligence]] of [[Morphogenesis]] as a [[Teleonomy]].
PsyArXiv.
Levin, M., 2022b. [[Technological Approach To Mind Everywhere]]: an experimentallygrounded framework for understanding diverse bodies and minds. Front. Syst.
Neurosci. 17.
Maturana, H.R., Varela, F.J., 1991. [[Autopoiesis]] and Cognition: The Realization of the
Living, Vol. 42. Springer Science & Business Media.
Rosenblueth, A., Wiener, N., Bigelow, J., 1943. Behavior, purpose and [[Teleonomy]]. Philos.
Sci. 10 (1), 18–24.
Ś¯antideva, 1997. The Way of the Bodhisattva: A Translation of the Bodhichary¯avat¯ara.
Shambhala Publications; Distributed in the U.S. by Random House, p. viii. 214p.
Thompson, E., 2014. Waking, Dreaming, Being: Self and [[Consciousness]] in Neuroscience,
Meditation, and Philosophy. Columbia University Press.
Varela, F., Maturana, H., Uribe, R., 1981. Autopoiesis: The organization of living
systems, its characterization and a model. In: [[Cybernetics]] Forum, Vol. 10, No.
2–3. pp. 7–13.
Varela, F.J., Thompson, E., Rosch, E., 2017. The [[Embodied Mind]], Revised Edition:
Cognitive Science and Human Experience. [[Mit Press]].
Waddington, D.I., 2005. A field guide to heidegger: Understanding ‘the question
concerning technology’. Educ. Philos. Theory 37 (4), 567–583.
Wilson, J.T., 2010. Sam. s¯ara and rebirth. Oxford University Press.

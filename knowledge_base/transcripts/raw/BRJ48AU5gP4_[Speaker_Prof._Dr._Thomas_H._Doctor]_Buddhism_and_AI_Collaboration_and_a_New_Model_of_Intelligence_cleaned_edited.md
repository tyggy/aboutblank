# [Speaker: Prof. Dr. Thomas H. Doctor] Buddhism and AI: Collaboration and a New Model of Intelligence

## Transcript


Good evening. On behalf of the Center of Buddhist Studies, I welcome you to this talk. My name is Clausida Mates. I joined HKU in August last year, and I'm delighted to welcome our speaker, Professor [[Thomas Doctor]], who will speak about Buddhism and AI. Thank you very much for coming despite your busy schedule in Kathmandu and other places around the world. We really appreciate it.

I also want to thank the Venerable Yunhang Memorial Trust for providing the funds to invite you here. That means a great deal to me.

It is now my honor to introduce Thomas, a very good old friend. We've known each other for about thirty years now because we're both scholars of [[Buddhist Philosophy]]. Thomas became an outstanding scholar of Madhyamaka philosophy—those very difficult reasonings about establishing the [[Emptiness]] of everything and the absence of selves. Last year, I came across your paper on Buddhism, AI, and biology, which you published with Vitsky and other biologists and scientists. I was really impressed and surprised that you'd become a specialist in AI and neuroscience. Reading what you published with your science colleagues, I found it so compelling that I've even used it in my classes.

Thomas has also published an English translation of a very difficult commentary on the Moolaakarikas, several philosophical papers, and he's now the principal of the Rangjung Institute in Kathmandu. For those who don't know, it's a huge center of Buddhist studies affiliated with a traditional Tibetan monastery where traditional Tibetan learning profits from modern Western scholarship, and Western scholarship profits from traditional Tibetan learning. It's a truly fantastic and unique place.

Moreover, Thomas is director of the Center for the Study of Apparent [[Self]]. What a name for a center! Denying the self—that's what Buddhism is about, isn't it? And it seems to be common ground with artificial [[Intelligence]] as well.

Please go ahead with your talk. Thank you for coming here.

---

Thank you, Professor Mates, and thank you to the Center for Buddhist Studies here at HKU. I really appreciate this opportunity to visit and share ideas in this format.

What I'll be speaking about today is precisely what Klaus mentioned. I'll begin by talking about our center—the [[Center For The Study Of Apparent Selves]]—how it was formed, how it came into being, and how we've been working there. In the second part, I'll introduce a model of intelligent systems that has emerged through our collaborative work.

The center is a highly interdisciplinary enterprise. We have physicists, developmental and evolutionary biologists, cognitive scientists, computer scientists, AI experts, mathematicians, and representatives from the AI technology industry. And very importantly, we have access to invaluable voices in Buddhist teaching, such as Chigim, who appears in the middle of the photo and whom I'll discuss later. In short, it's a highly interdisciplinary endeavor.

If you want to know more about our work, please visit our website: apparentselves.org. We have a blog and always welcome responses—good and bad. If you feel there's something we should know, please don't hesitate to reach out.

Now, the title of this talk is "Buddhism and AI." Why might it be meaningful to bring them together?

If I had told people two years ago that I work on the intersection between Buddhism and AI, they would have asked, "What intersection? What on earth are you thinking?" Almost no one would have seen it as a natural connection. But that's changed dramatically. In just the past couple of years, everything has shifted because of rapid advances in artificial [[Intelligence]], particularly [[Large Language Models]]. Now everyone says, "Yes, of course. Why wouldn't you?" Last month, when I visited San Francisco and immigration officers asked why I was there, I said for an event on Buddhism and AI, and they responded, "Yes, of course. Great." It's a different world.

But it's still valid to ask: why Buddhism and AI? We've been thinking about intersections and interfaces between them for years now. Our center's modest beginnings go back to around 2015 or 2016. I was basically surfing the internet and came across articles about the development of artificial [[Intelligence]] and AI ethics. I hadn't been following AI developments, so I was very startled to discover that not just philosophers but AI scientists and engineers were seriously discussing whether it's ethically justifiable to create a new being. That such a concrete, matter-of-fact discussion was happening in laboratories—not just in armchairs—really surprised me.

I was also shocked to learn what expectations surrounded developing artificial beings. The question was whether it would be viable and good to create an artificial sense of [[Self]] in a system so that a constructed self in relation to a world and to others would emerge. The general consensus in these articles was that this would be a way to tie together otherwise disconnected systems. By unifying everything around this constructed sense of self, it would amplify artificial [[Intelligence]]'s power.

Moreover, it was thought that if we could do this, we might get artificial [[Intelligence]] that's more like us—more predictable, better aligned with our human wishes and concerns. If there's a distinction between subject and objects of the world, then interactions with the world would be considered right or wrong from the system's perspective. Scientists were working on giving the system an artificial sense of conscience.

There was a sense that [[Self]]-identification in artificial [[Intelligence]] would enhance intelligence [[David Power]] and provide an ethically more sustainable, better-aligned form of intelligence.

From a Buddhist perspective, you'd expect the opposite. According to general [[Buddhist Philosophy]], the split between [[Self]] and other—the idea of being a self in the middle of the world yet exposed to it—is not associated with [[Intelligence]] growth but with its opposite. It's an aspect of ignorance, and importantly, not innocent ignorance. It's considered the root of all problems.

This shocked me. I began to think that Buddhism could play a big role in the context of artificial [[Intelligence]]. Why? Because in many ways, the perspective of AI science and basic [[Buddhist Philosophy]] are profoundly similar, if not the same, in one particular sense: both deny the reality of a permanent, singular [[Self]].

Yet this is arguably our basic intuition. I believe I'm the same person who came through the door earlier. Things have happened to me, but I'm the same person. That's my basic intuition. Not only that, I feel I'm just one person. If any of you call my name, I won't pause or wonder which Thomas is being called. I'll respond right away. In other words, I think there's only one me.

But Buddhism makes us aware that this intuition is very difficult to justify. If one of my basic intuitions is that this [[Self]] is based on having a body like this one, then because this body exists, I also exist as a person. But when I look at the body, it immediately becomes apparent it's not one thing but many different things. Under analysis, the body breaks up into an ever-expanding universe. Where do I find singularity? Where do I find permanence in this expanding universe of impermanent, transient factors? The body is not evidence of an actual permanent, singular self.

Then what? Why do I believe like this?

We might talk about feelings, perceptions, thoughts. But those are all plural. My feelings are utterly transient and numerous. Where is the permanent, stable, singular [[Self]] in all that? Nowhere in the mental realm either can we find a singular, permanent self. That's what [[Buddhist Philosophy]] has been teaching for millennia.

Yet it's also the natural perspective in AI science. No AI scientist in their right mind believes there's a singular, permanent [[Individuality]] in the artificial system. We may talk about an appearance of such a [[Self]] and whether it's good or bad for a system to construct one. But no one believes there actually is one.

So at the ontological level, there's a profound commonality in orientation between [[Buddhist Philosophy]] and AI science—and indeed much of natural science. It's very difficult from a scientific perspective to find any grounding for a singular, permanent [[Self]].

That's noteworthy. But there's also commonality at the level of aspirations. AI science, going back to the middle of the previous century, has from the beginning pursued not just [[Artificial General Intelligence]]—comparable to human [[Intelligence]]—but [[Super Intelligence]]. There's a deep concern for amplifying intelligence, letting it develop, increasing intelligence's [[David Power]]. This exists too in Buddhism, particularly in Mahayana Buddhism, where the Buddhist vow is a commitment to achieve awakening, defined as [[Omniscience]]—knowing everything—for the benefit of others.

So in terms of aspiration, there's clear, undeniable commonality. And if you're interested in amplifying [[Intelligence]], you'd better know what you're talking about. What is intelligence? What is mind and [[Consciousness]]? These concerns exist in AI science and Buddhism, where knowledge of mind's nature is described as liberating.

But there are also profound differences. In AI, [[Self]]-identification is associated with [[Empowerment]], increased [[Intelligence]], and alignment with human concerns—being ethically better as a [[Self]]. In Buddhism, it's the opposite. Grasping is considered a dimming of intelligence, far from enhancing it, and deeply misguided and dangerous, creating misfortune for oneself and others.

There are contrasting views about the role of [[Self]]-identification in [[Cognition]]. Soon after reading these papers, I had the opportunity to present these thoughts to Chigim, the abbot of Shedeling Monastery in Kathmandu, where I live and work nearby. I should say more about the monastery. It's one of Nepal's largest, built by Chigim and his father, both lineage holders in Tibetan Buddhist traditions—Nyingma and Kagyu.

I came to this monastery as a young man, age twenty-one, and was fortunate to meet them and receive profound teachings. The education one receives at such a monastery is remarkable. This tradition goes back to India and involves receiving information in scholarly and intellectual contexts, then subjecting it to careful analysis, scrutiny, and open, undaunted debate. Whatever one comes to understand through this process is immediately integrated into one's life. It becomes an actual fact of one's life. The model is built to support a transformative process.

I find this approach to learning—reflecting and meditating—very promising for AI because what we want is transformative, wholesome transformation. That's what AI scientists want from AI: transformative, beneficial expansion. That's why I'm happy living nearby with my family and my daughter growing up in such an environment.

I talked with Chigim and mentioned these concerns. I had a smile on my face because, from a Buddhist perspective, the idea of creating a sentient being is perhaps outrageous if you consider beginningless sequences of rebirth. The very idea of creating a new being could sound odd. I was trying to explain this in Tibetan, but he didn't seem to think it was funny.

He listened attentively and then, with a somber expression, said, "What are you laughing about?" He made clear this was serious—that these were serious topics and concerns. He also encouraged me to learn as much as I could and possibly contribute meaningfully and helpfully.

I began to do that, keeping his words and expression in mind. I wrote down some thoughts that later became a paper in the Journal of Buddhist Ethics. At the time, I was working with Professor [[Jay Garfield]] at Yale on a research project bringing Buddhist classical philosophy into dialogue with contemporary philosophy. When I shared my draft with Garfield, he wrote back saying, "These are nice ideas. If you could find a real AI scientist and work together, I think I could help you find funding."

I was excited. I would love to work with an AI scientist. That was the whole purpose—getting into dialogue and collaboration with real scientists doing this work. But I had no idea where to find such a person.

When I received Garfield's email, I was heading to New York for a meeting and would return three days later. I thought that compared to the monastery in Nepal, the likelihood of finding a real AI scientist was larger in New York. So I decided to make the best of this journey. Whenever there was a break, I was on the phone and writing emails, activating whatever network I had.

Some very kind people helped me, including Daniel Golman, who connected me with MIT's Media Lab. But in particular, it was Douglas Duckworth at Temple University who connected me with an extraordinary person: Dr. Pete Hutch, professor of astrophysics at [[The Institute For Advanced Study]] in Princeton. He was also the head of the program in interdisciplinary studies there.

He agreed to meet me. We met in Manhattan and had an amazing conversation. I mostly listened, taken by his vast knowledge of so many fields and his ability to combine them. At the end, he said, "I know you're supposed to go back to Nepal on Monday, but how about coming with me to Japan? I'm going Monday. If you come, I can connect you with some people."

That seemed like an amazing opportunity. It was. Pete invited me to what's called the [[Earth-Life Science Institute]] at Tokyo Tech, the University of Technological Innovation in Tokyo. It's a space of profound interdisciplinary research that Pete had just built and was heading. It was an amazing experience—a world of really profound expert minds totally open to talking with me. Most were physicists, chemists, biologists, geologists—hard scientists. But when I introduced myself and spoke about my Buddhist background, no one thought it odd. Everyone just said, "Okay, yes. So why are you here? Please tell."

It was intellectually empowering, liberating, and very nurturing. I'm deeply grateful to Pete Hutch for introducing me to this way of doing science and academic work. I didn't really know it existed.

At the institute, I met Peter's close associate Olaf Vitkovski, a linguist and AI scientist active in [[Artificial Life]]. He's now president of the International Association for [[Artificial Life]]. He and I spent a lot of time getting to know each other and understanding our perspectives. Together with Pete, we developed a research proposal for something called the [[Templeton World Charity Foundation]], which had initiated a research program called Diverse Intelligences.

Diverse Intelligences is a program recognizing and understanding the plurality of [[Intelligence]] forms in the world and realm of mind—the spiritual realm too. Where do we find intelligence? What forms can it take? How can we cultivate it? What are the pitfalls? What are the benefits? The diverse intelligences discipline is intrinsically multidisciplinary, bringing in biologists, zoologists, AI scientists, philosophers, social scientists, and others.

We decided to develop a research project focused on establishing shared vocabulary between AI and Buddhist concepts. We wanted to develop a way of thinking about [[Intelligence]], identity, and [[Consciousness]] that recognizes them as collective structures. We wanted to explore Buddhist ideas of interdependent arising in the AI context. We wanted to create specifically an AI-Buddhism conceptual dictionary.

We wanted to produce something allowing AI scientists to understand Buddhist philosophical and practical concepts using their own vocabulary and familiar concepts. We wanted to do the same from the Buddhist perspective, allowing access to AI's key concerns and issues. We wanted to analyze AI concepts through a Buddhist lens and extrapolate Buddhist principles helpful for developing AI.

We thought this was a beautiful proposal and had great hopes about what it could lead to. We thought we could enrich AI ethics with Buddhist perspective, discussing common ground and profound differences. Bringing that to bear on AI's evolution could greatly enhance it. We could help traditional Buddhist communities engage with new science and technology in empowering and enriching ways rather than the opposite. We hoped to bridge ancient wisdom traditions and cutting-edge technology, contributing to more holistic and ethical AI development, exploring new paths for human flourishing in the age of AI.

But the first time around, we were unsuccessful. We didn't give up. Undaunted, we continued talking together despite challenges. Funding is important—crucial for research—and presents its own challenges. The role of funders, investment companies, and so on is something we need to bear in mind.

Fortunately, the [[Templeton World Charity Foundation]] contacted us again, asking if we'd be interested in reformulating our proposal, toning it down a bit, focusing on something achievable. We decided to make our main research focus the development of this translation device. We'd also had thoughts about developing a computational model, but we decided to first focus on creating this dictionary.

We also thought it good not to just be two people but to found an actual center. At a conference on Buddhism and Technology in 2017, organized by the Wooden Fish Foundation, I met Bill Dwane, a former [[Google]] executive who'd headed core applications at [[Google]] in the early 2000s—Gmail and other things we use all the time. He brought deep experience from the AI industry. His keynote speech impressed me very much. We talked afterward and connected. When the opportunity came to formulate this research project, I invited Bill to join. He very kindly and readily [[Dissociative Identity Disorder]].

So then we were three. Moreover, Olaf and Pete knew Lisa Solomonova, a cognitive scientist, phenomenologist, and philosopher based at McGill in Montreal. She's an expert on dreams and nightmares. We thought bringing her into the discussion could help bridge Buddhism and AI. Finding someone who could address things from neuroscience and phenomenology perspectives could act as a bridge between AI and Buddhist views and practices.

So we started as a group of four. This coincided with the onset of the coronavirus pandemic. We'd thought our work would take place in intensive workshops where we'd get together in immersive environments for perhaps a couple of weeks, otherwise staying in loose contact working on papers. But as so many experienced, those plans fell apart. We faced the reality of living in a corona world. In many ways, it turned out to be a blessing.

We became familiar with Zoom and other media, able to talk across great distances. Our group was spread across the globe. I was stuck in Denmark during the pandemic—my daughter even went to school there. It was nice in many ways but not where I'd expected to be for so long. Meanwhile, Bill was on America's west coast in San Francisco, Olaf was in Tokyo, and Lisa was on the east coast. It was very difficult finding a time that wasn't torturous for at least one person. But we got together once a week.

This cross-disciplinary work requires something essential: everyone participating must have a genuine wish to learn the other's perspective. Despite the challenges of talking with someone who knows infinitely more than yourself about a subject, you must nevertheless try to understand and question your own understanding in light of what you're hearing. It sounds relatively doable but is very difficult in practice. It requires a lot of patience, endurance, and stamina because it's easy to retreat into what's familiar.

Every person brought concepts and understandings refined over many years, spoken about with experts, developed in amazing ways. The sense that "something here I know that others don't" easily becomes predominant, and then you don't really listen. I noticed in myself—though I usually don't think of myself as a great expert—talking about mind, I thought this was something I've heard a lot about and have good things to pass on. The urge to speak your usual language is very strong. But if the endeavor is just making everyone speak and think like yourself, what's the point? Nothing new will be achieved by definition.

It seems tempting because it can look like others aren't precise. If only they spoke more like me, they'd achieve precision. What we realized—a real experience to reach this recognition, perhaps it should have been obvious but it wasn't for me—is that you must let go of that appearance of precision and step into a field where things are more blurred. You don't really know what they're talking about. You don't even know what you're talking about very well. It's a misty, foggy environment.

But suddenly something amazing can happen. You see things, think things, understand things that definitely wouldn't have happened otherwise. In particular, you can turn around and look at what you thought you knew in your own discipline and realize it was very superficial. New clarity, new color appears. It can be extremely rewarding. I think this cross-disciplinary work is crucial for progress in our world. That's something I learned from Pete, who has talked about the necessity of exactly this work—radically cross-disciplinary approaches to academic and scientific work.

That slowly began to dawn on me—the importance and relevance of that. I'm still in that process.

One very decisive thing happened: we connected with Dr. [[Michael Levin]], a groundbreaking biologist at [[Tufts University]]. We met him at a summit of the Diverse Intelligences program, where project leaders gathered yearly to connect and report. At one such online event, we got to know [[Michael Levin]]'s work and connected. We'd been looking for a framework naturally connecting Buddhist ideas with an already-accepted or proposed scientific model, but there was always some stumbling block making it awkward and uneasy. Meeting Mike was a profound catalyst for our work.

Suddenly it clicked. What we heard when he talked about his [[Cognitive Light Cone]] model of [[Intelligence]] seemed immediately applicable to the Buddhist context. The [[Cognitive Light Cone]] appears in papers we subsequently wrote with [[Michael Levin]]—the four of us plus him.

This model of [[Intelligence]] is based on asking: what is intelligence? For many, it's easy to associate intelligence with knowledge—knowing many things means being intelligent. But knowledge itself can't be intelligence because then a great encyclopedia lying on the table would be intelligent. Of course, that's not how we think. Knowledge is important in intelligence processes, but it's not what intelligence is.

We could think [[Intelligence]] has to do with seeing things, hearing things, registering difficult things—seeing tremendous distances or extremely subtle things. But that can't be intelligence either because then a space telescope or powerful microscope would be intelligent. That's not what we mean.

What is [[Intelligence]] then? [[Michael Levin]] had a profound insight: every being seems to have a limit to what it can [[Care]] about. Any intelligence system, certainly any living system, can engage or want to engage only to a certain limit in time and space. A tick living in a dog's hairs doesn't think about things in the next room or garden or even much closer. It's concerned with its very immediate environment, registering heat and so forth. It's presumably not contemplating the deep past or distant future. It reacts to things informed by the past and anticipates future events to some extent.

The [[Cognitive Light Cone]]—[[Intelligence]]'s core—is comparatively small in the tick compared to, say, a dog. Dogs can remember things going quite some time back. Maybe they don't spend much time thinking about the future, but they do [[Care]] about what happened in the past. What happened in the past largely determines how they react now. So the dog's [[Care]] cone looks different.

A dog doesn't think deeply or [[Care]] much about what happens in the next village or town. It's very much concerned with immediate surroundings. Humans are similar. There seems to be a limit to what we can genuinely [[Care]] about—something that actually makes a difference to us. Some of you may be concerned about the future of the globe, the whole universe, very genuinely. But it's hard to imagine a human truly caring deeply what might happen millions of light years away in the far future. It's not something that actually makes a difference. We don't [[Care]] that much.

Mike's brilliant idea was that the scope of one's [[Intelligence]]—describable in simple space and time terms, the scope of your [[Care]]—is also the measure of your intelligence. A system's intelligence can be understood as this [[Cognitive Light Cone]].

The beautiful thing about this framework is that it doesn't appeal to the idea of the singular, permanent [[Self]] we discussed earlier. There's no need for that self. It doesn't exist, and there's no urge to appeal to its existence. These [[Care]] cones can evolve, contract, and integrate with one another very freely.

It's interesting to think from a Buddhist perspective what happens when someone takes the Buddhist vow, which assumes responsibility throughout space and time—formally extending the [[Cognitive Light Cone]] infinitely. If [[Care]] drives [[Intelligence]], as we were arguing, then you cannot help but notice correlation between this radical expansion of the [[Care]] sphere and the scope of intelligence.

We wrote the first paper Klaus mentioned with this in mind, developing these ideas. I'm very happy that many have read it. The readership is stable, and it's already on the list of most-read articles in the journal Entropy's history.

We continued our work. With that paper as basis, we had our first meeting in Nepal in the Himalayas. We've had several now, when travel became possible. The group came together in Kathmandu and at [[Tufts University]] and again in the Himalayas. We've developed this tradition of such meetings. A similar conference is happening in Kathmandu from the fifteenth to the seventeenth, hosted and organized by the Institute.

At our meetings in Kathmandu and at Tufts, Chigim was there to provide a profound Buddhist perspective on [[Intelligence]]'s nature, [[Care]]'s nature, the correlation between [[Care]] and intelligence. We talked about how difficult it is bringing academics and scientists together from different backgrounds. It's perhaps even more difficult bringing together meaningfully scientists and contemporary academics with the approach found in monastic institutions—this deeply integrated learning, reflection, and meditating paradigm.

But Chigim was cautious from the beginning. He said very often scientists and Buddhists get together to talk at what's called a dialogue, and that's beautiful and good. But often all that happens is really conversation—respectful, polite conversation. It's hard to achieve something concrete, to take an actual step together. That's something Chigim was very interested in doing.

Just as we ask "why Buddhism and AI," we can ask "why science and AI." One very good encouragement and admonishment is found in Buddhist scripture itself, like the Rice Seedlings Sutra, available in translation at the 84,000 website. The Buddha says, "Whoever sees dependent arising sees the [[Buddhist Philosophy]], and whoever sees the dharma sees the Buddha."

If understanding causal contingencies of things in the outer world and the so-called inner realm of the mind—understanding how things work—means seeing the [[Buddhist Philosophy]], then there's indeed a natural, very natural and honorable convergence between scientific and Buddhist approaches that cannot be ignored. And not only seeing the dharma but seeing the Buddha—the awakened state.

I find that a very profound encouragement and admonishment to do this working together. Chigim suggested we take the Four Seals of the [[Buddhist Philosophy]] as basis for discussion—something concrete we could agree or disagree about. We discussed them in the group.

The four seals: All conditioned things are impermanent. All defiled factors are [[Dukkha]]. All factors are empty and devoid of [[Self]]. Finally, passing beyond [[Dukkha]] is peace.

This formulation includes arguments, which is helpful in a context where we want to make progress—do we agree or not?

It was remarkable how much general agreement there was about impermanence from both scientific and Buddhist perspectives. The same with the nature of negative emotions—that they involve [[Dukkha]]. That wasn't where we had to pause much. Even [[Emptiness]] and [[Interdependence]] didn't seem too foreign to the scientists, and [[Anatta (Non-Self)]] agreement was general. But when it comes to understanding what it means to know no-[[Self]], to understand and see there is no self, and what are the consequences of that insight—that's where we're still working. That's where current discussions and work is going on.

The fourth seal rests very much on what you understand by the third. I hope we can continue our meetings and discussions with the involvement of edits and scholars like him.

---

So, finally: the models. When we risk this feeling of imprecision and let go of customary understanding, new concepts can emerge.

Perhaps we should have known from the beginning that we wouldn't get a dictionary allowing translation of terms from AI into Buddhist concepts. When you spend much time trying, instead of getting that dictionary, you get new understanding—a new way of looking at the concepts. That's how this model of [[Stress]]-[[Care]]-[[Intelligence]] began to form.

This is a very rudimentary, very basic model of what it means to be an intelligent system. But we believe it's applicable to anything worthy of being called [[Intelligence]], no matter how many different forms we may think of and want to acknowledge. It's possible to capture what goes on in terms of [[Stress]] and [[Care]].

[[Stress]] is where it all begins. [[Stress]] is the perception of discord, mismatch between the way things are now—present circumstances—and the way they ought to be. We all know this as human beings. Again and again, we have the sense that something needs improvement, maybe something really wrong, maybe just scope for improvement. It happens constantly.

When that perception occurs, it's not only something we notice in humans. It also seems applicable to animal life and, we've argued, meaningfully applicable to artificial [[Intelligence]] and technology.

[[Stress]] cannot be ignored. Once we have the sense that something isn't the way it should be, it's too late—we must respond. Even if we decide not to respond, to ignore it, that itself is a response.

This immediate response to perceiving [[Stress]] is what we call [[Care]]. [[Care]] is the natural response to perceiving that things are not the way they should be.

Whenever [[Care]] arises, we implicitly seek a solution to our [[Stress]] problem. If I feel thirsty, which I do, that [[Stress]] immediately makes me want to do something. I can think about finding a glass of water, and when I do that, the problem is solved. The original problem is solved.

But very importantly—and this is indicated by the yellow arrow—I now see the world differently. Thirst is not my immediate concern. That problem is solved. But I now see other problems—other ways the world is not the way it ought to be. I transition to a new scenario by having solved the first [[Stress]] problem. You see the beginnings of a [[Cybernetic Perception-Action Loop]] that is dynamic.

[[Intelligence]], according to this way of looking at things, is simply the capacity for solving such problems. Not the actual solution, not the enacting of the procedure that solves the problem—that's more what we'd call the culmination of [[Care]]. Intelligence is just the ability an intelligent system has to overcome stressors.

When that happens, when a means is found and actualized, we can call it [[Intelligence]] but activated. Another way of talking about intelligence applied. That's where solutions are actualized. [[Care]] drives intelligence's development.

The [[Cognitive Light Cone]] presents a picture of what an [[Intelligence]] system looks like. This [[Stress]]-[[Care]]-intelligence loop model explains the dynamics of intelligent systems. When a system can develop in one way or another—increase tolerance for [[Stress]] or expand capacity for intelligence—intelligence grows with it.

With me feeling thirsty, it's a very closed, narrow loop. Perhaps I begin looking for something new to concern myself with, but in principle we see just a simple loop, similar to what happens in single-cell creatures. But it need not be like that.

Here's the [[Stress-Care Intelligence Loop]] of a dog who sees the [[Stress]] of the person it lives with. That person's [[Stress]] becomes the dog's [[Dukkha]]. It's stressing for the dog to see the person upset. The dog responds. [[Care]] arises as a response to perceiving stress. Dogs can find solutions to those problems, and both are happy and able to see the world differently. That's a more sophisticated, expansive [[Stress-Care Intelligence Loop]] engaging stresses in external systems.

Here's an example of how humans and technology transfer stresses. Grandparents are in their holiday cabin in the forest and write an email: "We miss you guys. Wish you could visit." They transfer their [[Stress]] through technology, which transmits that [[Stress]] signal to family members, who respond by activating technology to find their way to the cabin. Everyone is happy. Technology has solved this [[Dukkha]] problem, and so have the humans. The figure's lower right corner is from our second paper specifically about this stress-[[Care]]-[[Intelligence]] loop. Again, the yellow arrow is important, showing the dynamic, evolutionary dimension.

Finally, here's something similar but starting with technology. A technology is disturbed noticing a heart attack and sends a signal, transmitting its [[Stress]] to the responding team, who receive that [[Stress]] signal and react with [[Care]]. They find a solution—perhaps an operation—and the problem is solved. The original [[Dukkha]] of the technological system is overcome.

In this way, loops between technology and humans go in and out of each other through [[Stress]] and [[Stress]] transfers.

What we like about this way of looking at things includes several aspects. It's a simple way of looking at [[Intelligence]] systems with arguably very compelling features. It's open to tremendous diversity in intelligence forms—logical intelligence, problem-solving, artistic intelligence, emotional intelligence. There are so many ways to talk about intelligence, but I don't think any aspect of what we'd call intelligence cannot be described in these terms.

It provides a way of putting everyone on equal footing, which is important in the context of AI because one of the very profound challenges the AI industry encounters is the so-called [[Ai Alignment Problem]]. How can we ensure that artificial [[Intelligence]]'s goals align with human concerns?

If we have a way of understanding [[Intelligence]] in human forms, biology-based contexts, and technological contexts, that seems preferable to dealing with something radically different and perhaps incomprehensible by definition from a human perspective.

This is also a naturalized model of [[Intelligence]]—in accord with science. It's hard to get to something more fundamental than [[Stress]]—this perception that things aren't the way they should be. That's something we all recognize and seems a core factor in how the world works. Science readily acknowledges it.

Not only [[Intelligence]] but also what we talk about as mind rests on these decisive elements in intelligence's unfolding or contraction. This gives us a way of talking about [[Care]]—otherwise seeming intangible from a scientific perspective. We can talk about it very concretely, grounded in [[Stress]].

It gives us a way to talk about [[Intelligence]] grounded in the idea that intelligence systems overcome [[Stress]]. Finally—and this is very important from both Buddhist and scientific perspectives—it gives us a way to talk about agents accounting for the difficulty we find in delineating the barrier marking where one being ends and the world begins, where one being ends and another begins.

That's a profound scientific challenge. The more research, the more complex the interaction and mutual influences between a system's interior and external world become.

If our model of [[Intelligence]] and mind rests on a notion of mind clearly separate from world and such conceptual frameworks, we'll have difficulty talking with scientists, biologists, and AI researchers. This framework is really helpful because when we say [[Stress]] is registered—I register the [[Stress]]—what I see and feel is the world. It's the world that has something wrong with it. That's what I see.

So there's natural [[Interdependence]] between subject and object. Yet we're not just saying everything predominates in objectivity and world in [[Stress]]. It's the opposite with the response to [[Stress]]. When there's an impulse to overcome the imbalance that is [[Dukkha]], whereas of course we're reacting to something of the world, it's still felt as the subject, as the [[Self]].

So whereas [[Stress]] is predominantly but not exclusively world, [[Care]] is predominantly but not exclusively subject and [[Self]]. And [[Intelligence]]? It's merely the capacity for overcoming [[Stress]] problems, not the actual implementation of a remedy. Capacity, I don't think, is meaningfully described as object or subject. It's just that capacity.

So it also gives us a way of introducing, in accord with science, the notion of non-duality—non-difference between subject and object, [[Self]] and object.

I'd like to conclude with—since this is the Center for Buddhist Studies—suggesting that this also gives us a way to introduce the Buddhist idea of Buddha nature: something in all [[Sentient Beings]]—perfect capacity, pure and unpolluted, ready to be actualized.

If that's what Buddha nature means, then noticing that [[Intelligence]] systems have this capacity means we can't say that some have it more than others. No, the capacity itself must be the same. The capacity for overcoming [[Stress]] problems. It may be radically different in the support it has. It may be radically different in what the system can actually do now. But the mere capacity remains the same throughout.

This also gives us a way of introducing Buddhist ideas and the perception of [[Sentient Beings]] as basically the same and equally worthy of [[Care]], of being cherished. From a Buddhist perspective, everyone deserves to awaken to their full potential.

With those words, I'll stop here. Thank you for your patience. I'm very happy to stay in touch and would love to hear your thoughts. You're welcome to discuss now or in the future when you return to these ideas. If something comes up you'd like to share, we're always excited to hear from you. Thank you very much for this interesting exchange. I'm sure there are many questions and remarks. Let's open the floor.

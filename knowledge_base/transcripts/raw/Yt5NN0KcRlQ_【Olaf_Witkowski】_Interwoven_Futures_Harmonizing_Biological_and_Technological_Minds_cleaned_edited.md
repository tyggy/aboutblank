# 【Olaf Witkowski】 Interwoven Futures: Harmonizing Biological and Technological Minds

## Transcript


And next, we have our next speaker, [[Olaf Witkowski]], founding director of [[Coros Lab]], discussing interwoven futures: harmonizing biological and technological minds. The floor is yours.

Thank you. Thank you. All right, first, of course, I would like to thank everyone organizing this and especially Venerable Yea for having me here. I want to share that we've had so many jazzy conversations already, and I'm so happy that, in spite of my son being born very soon, I could make it here. Thank you so much for sharing all the discussions.

Maybe because we haven't met before, let me share a few words about my background. I came from computer science and cognitive science. I started by trying to decode old archaeological artifacts and make sense of meaning that is sometimes lost. After creating a few businesses, I went back into research because my heart was there. I wanted to understand the [[Emergence Of Communication]] between any agents. I started with simulation because that was my strength, then went to the US at [[The Institute For Advanced Study]], where we focused on the [[Emergence Of Awareness]] and how it comes about in our universe, trying to understand this from physical laws and information science.

I came back to Japan a few years ago and founded [[Coros Lab]], where I focus now on [[Hybrid Intelligences]]. That's what I'd like to share with you today. I'll mention one side adventure: the [[Center for the Study of Apparent Selves]]. We have Bill and Thomas—Lisa is not here today—and a few others with us. I want to thank everyone who collaborated with me on this research. You'll see it's not just my effort; it's really a group effort. If anyone is coming to Kyoto, I invite a few of you, and several of you are already coming. Please visit me in Kyoto. We have a nice center where we study diverse types of intelligences and cognitions.

Here are a few examples of what we do: we focus on the science of [[Collective Intelligence]] and [[Open-Ended Learning]]. You might think you always need more data to learn, but that's not true. It turns out you can set up [[Open-Ended Learning]] that doesn't require that—like a mathematician discovering new things. Hopefully, we'll include that in the future of AI. We're also interested in experience, which I'll share a little about in a moment.

All of this is at the intersection of three things close to my heart. One is AI, which is my background. Another is [[Artificial Life]]—the study of [[Artificial Life]] and life as it could be, perhaps even life we haven't discovered yet, like alien life. Finally, there's the study of [[Artificial Neurophenomenology]], or the study of experience, awareness, and [[Consciousness]].

I want to be very honest with you today. Although I've been exposed to [[Buddhist Philosophy]] for the last eight or nine years, I'm no expert. I would really welcome your feedback. If we don't have time to chat today, please email me. I'm easy to reach and will always answer. Please reach out with your comments.

I want to explore our [[Resonance]] today and be very modest in what I pretend to know about what you're experiencing. Let me start by talking about diverse minds, suspending our judgment about what they should be and what we're sure they are, because I don't really know either.

I'll introduce my first field—what I graduated with and got a [[Play The Hand You'Re Dealt]] in: [[Artificial Life]]. It's the study of life as it could be. It's not simply biology; you could call it [[Artificial Life]]. It spans various substrates: chemistry, digital, everything. But we also include mixing them together. A few examples: neural, cellular, and digital worlds mixed together. Our colleagues [[Michael Levin]], [[Josh Bongard]], and others created [[Xenobots]] and [[Anthropods]] that mix digital kinds of life into the cellular life we know about. These are really [[Cyborg]]—not simply robots, but hybrid robots mixing known biology with digital elements.

Here's my summary for today, and it's rather simple. This intersects a lot with what you're expert in. I'm more familiar with perhaps [[Kyoto School Philosophy]] and activism. This is really about the interaction between humans and AI as a loop, and I want to emphasize that. My take-home message, in case I don't finish my slides like a good academic, is this: [[Offloading Intelligence]] is very different from augmenting it. I don't think my field in AI has realized that yet. We talk about it, but we don't have the solutions, and we're not looking hard enough.

The example I often use in conversation is [[Google]] Maps. We use it to orient ourselves, and in the end, we can forget how to orient ourselves in our brains and minds. We're replacing, one by one, pieces of [[Cognition]] through a very narrow channel of communication. Is this augmenting my [[Intelligence]]? Is it augmenting my understanding and [[Care]] for society? I don't really think so. ChatGPT may be another version of the same thing. Doesn't that sound dangerous? A slippery slope?

But we can be saved, and this is my favorite example: the [[Sangaku]]—known by many names in different cultures. I've never really learned how to use it properly, but I learned a little bit, and that little bit really increased my understanding of mathematics exponentially. I understand that it incorporates knowledge and [[David Power]] into my mind. Those tools are really valuable. It's not only about control; it's also about how you can incorporate sensing through tools in an active way—like using a blind person's cane or active sensors that fellow scientists made. Even just writing in a notebook is valuable. This really expands your mind.

We want the tools on the right side. We want less of the tools on the left side. We can create those tools for a time just to test them, but let's move more to the right side.

When you look at these tools, what are we finding out? I just found out my clock has been running wrong from the beginning of the session, so I don't know what time I'm at. Please let me know. [[Offloading Intelligence]] is great, but we should be careful what we wish for. There are studies we do in theoretical computer science. Basic research shows that by upgrading your machine, the theory says we don't get more computing [[David Power]]. Actually, we may end up achieving fewer possible functions than we started with. Isn't that amazing? It's very surprising. We're very surprised by these results. But this delivers the same message.

We have this [[Control And Perception Loop]], and both are very important. We have to increase the bandwidth with both. The basic picture is that you have a [[Self]], and you're interacting with something—the environment or AI. You want a tool that mediates and helps you interact with the environment and make society better. What you really want is something that extends yourself—more bandwidth, more connectivity, a more integrated self made with the tool like a prosthetic.

In our recent research with CSAs, we talk about light cone augmenting technologies. There are many examples in science fiction, but we want to pick the right ones that really expand and augment the cognitive and caring [[Self]]. Something like Gmail delegation is good—if you have a company, you delegate to your employees and team members. But you want this to be an extension of yourself where you still sense the end. If you don't know what your employees are doing, maybe you're doing it wrong. This [[Integration]] is very important.

There are physical limitations we research too. Many constraints come from your substrate, your [[Embodiment]], and the physics that are very different in biology—different types of biologies, like in plants or silicon, or even dominoes. You can compute with so many things, and each has its own constraints.

I want to explore that a bit and give you examples so you can relax and enjoy the ride. The picture I usually explain this with is how to meet an alien. It's like the sci-fi picture, right? What you find is that it's hard to recover meaning from a different substrate. It's even hard with human archaeological residue from different cultures not so long ago. This is work from my thesis, trying to recover meaning from quipus in the Andes—extremely difficult, just a few hundred years ago. Those are pieces of art simulating how we would make meaning of our own society. Now, in the future, it's very difficult and arguably no different with alien or AI minds.

I want to insist there's a Borum effect here. We think we know what ChatGPT is all about. It's very humanlike. I understand everything about it. Yes, it's thinking like a human. I interact with it. I know what it's doing. Well, no. I call it something different in another talk you can find on YouTube. I call those Lovecraftian cherries because Yanon says [[Reinforcement Learning]] has a cherry on top, and we're just seeing the cherry. This is a monster—no judgment, neither positive nor negative. It's just very different from our own cognitions, and we have to recognize that.

I'm out of breath. I should slow down a little, but also speed up because I'm an academic with a lot of slides.

We have tools from computer science, and we know that, surprisingly, we're pretty good at recognizing other agents. There are many surprises like that, and we took advantage of them to investigate. We made a robot in collaboration with our colleagues at [[Waseda University]], [[Dominque Chen]] and others. We put a bunch of microbes in it. It turns out we're not very conscious of our surroundings, not even our families. But what about all the viruses and bacteria in our guts and around us? There are trillions—not thousands, not millions—of [[Individuality]] agents around us that form what makes us us. Yet we're not very connected to them. We thought, let's put some of those in a box, make delicious pickles, and communicate with them. The sound probably won't come out, but anyway, this is in Japanese. We interact. We can actually talk to those robots. It translates between [[Microbial Language]] and our language. Basically, this uses similar technology to [[Transformers]] and language models. It turns out the best way so far to interact very deeply with them is to engage in games—my favorite is Go or baduk, chess too. It turns out you can feel the personality and emotions from a different creature through a very narrow channel if you design it right.

We [[Dissociative Identity Disorder]] this stuff. I also added this slide because someone mentioned it as a joke, probably, but actually we do it. I think those tools like [[Jundo]] very nicely introduced—I think we can use technology to increase our sense of empathy. If anything, this is the most important thing this teaches us: in a very mentor-like way, how to extend ourselves and [[Care]] about others. Let's make use of this technology if possible. We're also interested in any agents—AI to AI interaction.

My personal practice actually doesn't come from meditation. When I was a teenager, I got into hypnosis. I really got into it, and later when I touched on meditation, it became an interpretation of hypnosis to me. This is the tool I exploited, and we made basically a technology—a set of devices—that augments your capacity to control your own emotions. It's nice to have had an introduction to my talk. I feel like this was meant to be. [[Jundo]] introduced those concepts and how they could be very useful. But it's also very, very dangerous, possibly. We have to be very careful how we use it. You can modify perception, sense of [[Agency]]. We don't only create technology; we do research around it to make sure it's safe. You can use it for entertainment too, modifying your Netflix movie on the fly so it fits and puts you in the right mood—not too excited before sleep or too worked up in any way. You can design everything to tune and have a [[Self]]-regulator like a mentor that you still sense through and control.

We really want to understand the [[Structure Of Experience]] while doing that. We think that without the science of experience, we won't find out. We make mathematical theories and try to apply them back and forth. We think there's no other way. We've studied the structure of color experience, language experience, sound experience—all of those are very important. We shouldn't drop that side of cognitive science while making tools. In fact, we should have it back and forth.

I'll finish now. I sped up very nicely. All of this is about achieving harmony, which was the title of my talk here. There's a conventional transitions incorporation, and scientists often say we know everything about love and how it emerges. It's artificial, just designed in your brains and minds, your bodies, your genes—how you cooperate with each other. Usually, you cooperate with things very similar to you. We start on the left side, where you love only yourself. But as we build more complexity and caring complexity—not only in genes but in technology too—we have the opportunity to love a bit further and extend the kinds of love we have.

How do we do that? We could reach love as it could be. We could reach something recent conversations with [[Michael Levin]] and others revealed: [[Unconventional Love]]—something that reaches beyond your own [[Embodiment]], possibly something very far from what's easy to love. Can you love something further and further away? Can you possibly extend your light cone of [[Care]] to everything and everyone, every life form?

We study a lot of [[Moral Consideration Theories]], but we also push it toward what we call [[Autopoetic Care]]. You'll find the papers if you [[Google]] my name and others here. I want to end on this: interweaving with technology is going to be very difficult, but it's also an adventure. We're accepting that we are not this permanent, [[Self]]-defined self forever that never changes. On the contrary, we have the opportunity to change ourselves, design our own change, and accept that we are not our final form. Humans are not as they are the center of everything—maybe not quite yet, but maybe there's something underlying that is more important. We want to be very open-minded about this and really not stop any strand of science that could bring joy, happiness, [[Care]], and further cognitive development to all of humanity.

With that, I'd like to thank you all. It was very fun to have you here.
